# CASE: Conflict-Aware Synthesis of Evidence

This repository contains code and artifacts for **CASE (Conflict-Aware Synthesis of Evidence)**, a project that explores how multi-agent Retrieval-Augmented Generation (RAG) systems can better handle **legal queries involving conflicting precedents and ambiguity**.

Legal reasoning is inherently adversarial: many questions do not have a single authoritative answer, but instead require weighing competing interpretations supported by different pieces of evidence. Standard RAG pipelines are poorly aligned with this setting, as they are optimized to retrieve and present one “best” answer, often oversimplifying conflicts or hallucinating unsupported resolutions.

CASE is designed to **embrace legal conflict rather than avoid it**, modeling the structure of courtroom-style reasoning through a multi-agent pipeline that retrieves and evaluates evidence for competing hypotheses before synthesizing a final answer.

For full methodology, experiments, and analysis, see **CASE Report.pdf** included in this repository.

---

## Motivation

Conventional RAG systems struggle in legal domains for two main reasons:
1. They tend to retrieve evidence supporting only one side of an argument, ignoring conflicting precedents.
2. When ambiguity remains, they often rely on the language model’s internal knowledge, leading to ungrounded or factually incorrect answers.

This project asks whether a **conflict-aware, multi-agent RAG pipeline**—explicitly designed to retrieve and reason over competing claims—can produce answers that are more grounded and faithful to legal reasoning than standard single-path RAG systems.

---

## Overview of the CASE Architecture

CASE is a three-stage multi-agent pipeline inspired by adversarial legal reasoning:

### 1. Hypothesis Generation
An analyst agent processes the user’s legal query and generates a set of competing legal hypotheses.  
Depending on the task, hypotheses are either:
- directly derived from multiple-choice answer options, or
- generated by the model for open-ended legal questions.

These hypotheses represent the competing legal claims that must be evaluated.

### 2. Advocate Retrieval
For each hypothesis, an independent **Advocate agent** is spawned with the explicit goal of constructing the strongest possible argument *for that hypothesis*.

Each advocate:
- reasons about its assigned claim,
- retrieves evidence using a combination of tools (BM25, Query Likelihood Model, dense retrieval, and optionally web search),
- iteratively refines its retrieval strategy, and
- produces a curated set of evidence snippets supporting its claim.

### 3. Arbiter Synthesis (Jury-Based Decision)
To resolve the conflict among advocates, CASE uses a **persona-based jury mechanism**.

A small panel of legal personas is sampled for each query, including perspectives such as:
- strict textual interpretation,
- precedent consistency,
- practical consequences,
- equity and fairness,
- adversarial skepticism.

Each juror independently evaluates the competing hypothesis–evidence pairs and casts a vote.  
The final answeror answer is determined by **majority consensus**, reflecting how legal decisions are often reached under disagreement rather than certainty.

---

## Baselines and Comparisons

To contextualize CASE, the project evaluates it against several baselines:
- LLM-only (no retrieval)
- Sparse RAG (BM25, Query Likelihood Model)
- Dense single-path RAG
- Self-RAG
- L-MARS (a prior multi-agent RAG system)

All baselines are evaluated under a standardized setup to ensure fair comparison.

---

## Datasets and Evaluation

Experiments are conducted on the **Bar Exam QA** benchmark, which consists of multiple-choice legal questions paired with a large corpus of legal passages. This dataset is particularly challenging because:
- relevant evidence often has low lexical overlap with the query,
- questions frequently involve subtle distinctions and conflicting rules,
- correct answers require legal reasoning rather than surface matching.

Evaluation focuses on **answer accuracy**, rather than retrieval recall, since retrieval in multi-agent systems is dynamic and adaptive rather than a single fixed step.

---

## Results (High-Level Summary)

Results show that:
- Standard RAG baselines often achieve accuracy comparable to LLM-only systems, suggesting limited reliance on retrieved evidence.
- Simply increasing retrieval breadth does not guarantee better performance and can introduce noise.
- CASE benefits from explicitly modeling conflict: combining multiple retrieval strategies and adversarial advocates improves accuracy, particularly in uncertain cases.
- The jury-based arbitration mechanism is especially effective in **low-confidence scenarios**, where competing interpretations exist and single-path reasoning is most likely to fail.

Overall, CASE demonstrates that **reasoning over conflict—rather than suppressing it—leads to more grounded and defensible answers** in legal question answering.

Detailed quantitative results, analysis, and ablations are provided in **CASE Report.pdf**.

---

## Repository Contents

This repository includes:
- Implementation of the CASE multi-agent pipeline
- Baseline RAG systems and retrieval variants
- Experiment and evaluation code
- The full project report: **CASE Report.pdf**

Trained models, large indices, and external datasets are not included.

---

## Notes

This is a semester project for CS 646 Information Retrieval at UMass Amherst.

For full experimental details, architectural diagrams, and discussion of limitations, please refer to **CASE Report.pdf**.
